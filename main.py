import os
import json
import hashlib
import smtplib
import requests
import feedparser
from datetime import datetime
from openai import OpenAI
from email.mime.text import MIMEText
from email.header import Header
from email.utils import formataddr
from bs4 import BeautifulSoup

# ================= 1. é…ç½®åŒºåŸŸ =================

API_KEY = os.environ.get("API_KEY")
API_BASE_URL = "https://api.deepseek.com"

SENDER_EMAIL = os.environ.get("SENDER_EMAIL")
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD")
# å¤šä¸ªæ”¶ä»¶äººæ”¯æŒ
RECEIVER_EMAILS = os.environ.get("RECEIVER_EMAIL", "").split(",")

HISTORY_FILE = "news_history.json"

# ğŸ”¥ ä¼˜åŒ–åçš„ä¿¡æºï¼šç§»é™¤ 404ï¼ŒåŠ å…¥å·¨å¤´åŠ¨å‘
RSS_SOURCES = {
    # 1. ç¡¬æ ¸ AI (Paper & Big Tech)
    "HARDCORE_AI": [
        "http://export.arxiv.org/rss/cs.AI",  # ArXiv AI
        "https://openai.com/news/rss.xml",  # OpenAI
        "https://research.google/blog/rss",  # Google DeepMind/Research (æ›¿ä»£ PyTorch)
        "https://www.microsoft.com/en-us/research/feed/",  # Microsoft Research
        "https://huggingface.co/blog/feed.xml",  # Hugging Face
    ],
    # 2. ç¤¾åŒºçƒ­è®® (Reddit = æœ€ä½³çš„ Twitter å¹³æ›¿)
    "COMMUNITY_BUZZ": [
        "https://www.reddit.com/r/LocalLLaMA/top/.rss?t=day",  # æœ€ç¡¬æ ¸çš„å¤§æ¨¡å‹ç¤¾åŒº
        "https://www.reddit.com/r/MachineLearning/top/.rss?t=day",
        "https://news.ycombinator.com/rss",  # Hacker News
    ],
    # 3. æ·±åº¦å›é¡¾
    "TECH_INSIGHTS": [
        "https://www.theverge.com/rss/index.xml",
    ]
}


# ================= 2. çˆ¬è™«å·¥å…· =================

def get_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}


def save_history(history):
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def clean_text(html_content):
    if not html_content: return ""
    soup = BeautifulSoup(html_content, "html.parser")
    # å»é™¤ä»£ç å—ç­‰å¹²æ‰°ï¼Œåªç•™æ–‡æœ¬
    text = soup.get_text(separator=' ', strip=True)
    return text[:600] + "..."  # ç¨å¾®å¢åŠ é•¿åº¦ç»™ AI åˆ†æ


def fetch_data():
    print("ğŸ•·ï¸ æ­£åœ¨æŠ“å–å…¨çƒæƒ…æŠ¥...")
    history = load_history()
    today_str = datetime.now().strftime("%Y-%m-%d")

    # å†å²è®°å½•ä¿ç•™ 3 å¤©
    valid_history = {k: v for k, v in history.items()
                     if (datetime.now() - datetime.strptime(v, "%Y-%m-%d")).days < 3}

    collected = []

    # ä¼ªè£… Chrome
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    }

    for category, urls in RSS_SOURCES.items():
        print(f"  ğŸ‘‰ æ‰«æ: {category}...")
        for url in urls:
            try:
                resp = requests.get(url, headers=headers, timeout=20)
                if resp.status_code != 200:
                    continue  # é™é»˜è·³è¿‡é”™è¯¯æº

                feed = feedparser.parse(resp.content)

                # é™åˆ¶æ¡æ•°ï¼šAIç±»å¤šå–ä¸€ç‚¹ï¼Œå…¶ä»–å°‘å–ä¸€ç‚¹
                limit = 3 if category == "HARDCORE_AI" else 2

                for entry in feed.entries[:limit]:
                    link = entry.link
                    uid = get_hash(link)

                    if uid in valid_history: continue
                    valid_history[uid] = today_str

                    content_raw = ""
                    if hasattr(entry, 'content'):
                        content_raw = entry.content[0].value
                    elif hasattr(entry, 'summary'):
                        content_raw = entry.summary
                    elif hasattr(entry, 'description'):
                        content_raw = entry.description

                    collected.append({
                        "category": category,
                        "title": entry.title,
                        "url": link,
                        "summary": clean_text(content_raw),
                        "source": feed.feed.title if hasattr(feed.feed, 'title') else "Web"
                    })
            except Exception as e:
                print(f"    âŒ Err: {url} -> {e}")

    return collected, valid_history


# ================= 3. AI æ ¸å¿ƒé€»è¾‘ (ä¿®å¤æ‚è´¨) =================

def generate_newsletter(items):
    print(f"ğŸ§  AI æ­£åœ¨æ·±åº¦åˆ†æ {len(items)} æ¡æƒ…æŠ¥...")
    if not items: return None

    data_str = ""
    for i, item in enumerate(items, 1):
        data_str += f"[{i}] <{item['category']}> {item['title']}\næ¥æº: {item['source']}\nå†…å®¹: {item['summary']}\né“¾æ¥: {item['url']}\n\n"

    client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)

    # ğŸš€ Prompt é‡ç‚¹ä¿®æ”¹ï¼šç¦æ­¢è¾“å‡ºä»»ä½•é HTML å†…å®¹
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸ä»…æ‡‚æŠ€æœ¯ï¼Œè¿˜æ‡‚ä¼ æ’­å­¦çš„ç§‘æŠ€ä¸»ç¼–ã€‚è¯·æ ¹æ®ç´ æç¼–å†™ä»Šå¤©çš„æ—¥æŠ¥ã€‚

    ã€ç»å¯¹æŒ‡ä»¤ã€‘
    1. **åªè¾“å‡º HTML ä»£ç **ã€‚ä¸è¦è¾“å‡º "å¥½çš„ï¼Œè¿™æ˜¯æ—¥æŠ¥" æˆ–è€… "ç¬¬ä¸€éƒ¨åˆ†" è¿™ç§åºŸè¯ã€‚
    2. ä¸è¦è¾“å‡º markdown æ ‡è®°ï¼ˆå¦‚ ```htmlï¼‰ã€‚

    ã€å†…å®¹ç»“æ„è¦æ±‚ã€‘
    è¯·æŒ‰ç…§ä»¥ä¸‹é¡ºåºç”Ÿæˆä¸‰ä¸ª `div` æ¿å—ï¼š

    **æ¿å—ä¸€ï¼šTL;DR (æ‘˜è¦)**
    - HTMLç»“æ„: `<div class="section-tldr">...</div>`
    - å†…å®¹ï¼šç”¨ `<ul><li>` åˆ—è¡¨åˆ—å‡ºä»Šå¤©æœ€é‡è¦çš„ 3 ä¸ªæ ¸å¿ƒçœ‹ç‚¹ï¼ˆç”¨ Emoji å¼€å¤´ï¼‰ã€‚

    **æ¿å—äºŒï¼šHardcore AI (ç¡¬æ ¸æŠ€æœ¯)**
    - HTMLç»“æ„: `<div class="section-news"><h2>ğŸ§  ç¡¬æ ¸ AI & å‰æ²¿</h2> ...å…·ä½“æ–°é—»... </div>`
    - å†…å®¹ï¼šæŒ‘é€‰æœ€ç¡¬æ ¸çš„è®ºæ–‡ã€å¼€æºæ¨¡å‹ã€å¤§å‚åŠ¨æ€ã€‚
    - æ¯æ¡æ–°é—»ç”¨ `<div class="news-item">...</div>` åŒ…è£¹ã€‚

    **æ¿å—ä¸‰ï¼šCommunity Buzz (ç¤¾åŒºçƒ­è®®)**
    - HTMLç»“æ„: `<div class="section-news"><h2>ğŸ”¥ ç¤¾åŒºçƒ­è®® (Twitter/Reddit é£å‘)</h2> ...å…·ä½“æ–°é—»... </div>`
    - å†…å®¹ï¼šæŒ‘é€‰æœ€æœ‰äº‰è®®ã€æœ€æœ‰è¶£çš„ç¤¾åŒºè®¨è®ºã€‚è¯­æ°”è¦åƒæ¨ç‰¹å¤§Vç‚¹è¯„ä¸€æ ·çŠ€åˆ©ã€‚

    ã€å•æ¡æ–°é—» HTML æ¨¡æ¿ã€‘
    <div class="news-item">
        <h3 class="title"><a href="URL" target="_blank">ä¸­æ–‡æ ‡é¢˜</a></h3>
        <div class="meta">
            <span class="source">æ¥æºåª’ä½“</span>
            <span class="read-time">é¢„è®¡é˜…è¯» 2min</span>
        </div>
        <p class="summary">
           è¿™é‡Œæ˜¯å†…å®¹æ‘˜è¦ã€‚å¦‚æœæ˜¯æŠ€æœ¯æ–‡ç« ï¼Œè¯·è§£é‡Šå®ƒç‰›åœ¨å“ªé‡Œï¼›å¦‚æœæ˜¯è®¨è®ºï¼Œè¯·æ¦‚æ‹¬æ­£åæ–¹è§‚ç‚¹ã€‚
        </p>
    </div>

    ã€ç´ æã€‘
    {data_str}
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=4000
        )
        content = response.choices[0].message.content
        # äºŒæ¬¡æ¸…æ´—ï¼Œé˜²æ­¢ AI ä¸å¬è¯
        content = content.replace("```html", "").replace("```", "").strip()
        # å¦‚æœ AI è¿˜æ˜¯è¾“å‡ºäº† "æ¿å—ä¸€" è¿™ç§å­—ï¼Œå¼ºåˆ¶å»æ‰ï¼ˆé€šå¸¸ DeepSeek å¾ˆå¬è¯ï¼Œä¸ç”¨æ­£åˆ™ä¹Ÿè¡Œï¼‰
        return content
    except Exception as e:
        print(f"AI Error: {e}")
        return None


# ================= 4. é‚®ä»¶è§†è§‰å‡çº§ (é«˜å¯¹æ¯”åº¦) =================

def send_email(html_body):
    print("ğŸ“§ æ­£åœ¨å‘é€...")

    # ğŸ¨ CSS è§†è§‰å¤§æ”¹ç‰ˆï¼šé«˜å¯¹æ¯”åº¦ã€çº¯é»‘æ–‡å­—
    css = """
    <style>
        /* å…¨å±€é‡ç½® */
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; background-color: #f6f8fa; color: #1a1a1a; margin: 0; padding: 20px; line-height: 1.6; }
        .container { max-width: 680px; margin: 0 auto; background: #ffffff; border-radius: 12px; padding: 30px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); }

        /* å¤´éƒ¨ */
        .header { text-align: center; border-bottom: 2px solid #000; padding-bottom: 20px; margin-bottom: 30px; }
        .header h1 { font-size: 26px; font-weight: 900; letter-spacing: -0.5px; margin: 0; text-transform: uppercase; }
        .header p { color: #555; font-size: 14px; margin-top: 5px; }

        /* TL;DR æ¿å— (é»„è‰²èƒŒæ™¯é«˜äº®) */
        .section-tldr { background-color: #fff8c5; border: 1px solid #e1d68d; border-radius: 8px; padding: 15px 20px; margin-bottom: 30px; }
        .section-tldr ul { margin: 0; padding-left: 20px; }
        .section-tldr li { margin-bottom: 8px; font-weight: 600; color: #24292f; }

        /* æ–°é—»æ¿å—æ ‡é¢˜ */
        h2 { font-size: 20px; font-weight: 800; border-left: 5px solid #0366d6; padding-left: 10px; margin-top: 40px; margin-bottom: 20px; color: #000; }

        /* å•æ¡æ–°é—»å¡ç‰‡ */
        .news-item { margin-bottom: 25px; padding-bottom: 20px; border-bottom: 1px solid #eaeaea; }
        .news-item:last-child { border-bottom: none; }

        /* æ ‡é¢˜é“¾æ¥ (å¼ºåˆ¶çº¯é»‘ï¼Œç‚¹å‡»åä¸å˜è‰²) */
        .title { margin: 0 0 8px 0; font-size: 18px; line-height: 1.4; font-weight: 700; }
        .title a { color: #000000 !important; text-decoration: none; border-bottom: 1px solid #ddd; transition: all 0.2s; }
        .title a:hover { color: #0366d6 !important; border-bottom: 2px solid #0366d6; }
        .title a:visited { color: #000000 !important; } 

        /* å…ƒæ•°æ® */
        .meta { font-size: 12px; color: #666; margin-bottom: 8px; display: flex; gap: 10px; }
        .source { background: #f1f3f5; padding: 2px 6px; border-radius: 4px; font-weight: 500; }

        /* æ‘˜è¦ (åŠ æ·±é¢œè‰²) */
        .summary { color: #333333 !important; font-size: 15px; margin: 0; text-align: justify; }

        .footer { text-align: center; font-size: 12px; color: #999; margin-top: 40px; border-top: 1px solid #eee; padding-top: 20px; }
    </style>
    """

    full_html = f"""
    <html><head>{css}</head><body>
        <div class="container">
            <div class="header">
                <h1>AI Insider Daily</h1>
                <p>{datetime.now().strftime('%Y.%m.%d')} | Hardcore Tech & Community Buzz</p>
            </div>

            <!-- AI ç”Ÿæˆçš„å†…å®¹ç›´æ¥åµŒå…¥è¿™é‡Œ -->
            {html_body}

            <div class="footer">
                Served by DeepSeek â€¢ GitHub Actions
            </div>
        </div>
    </body></html>
    """

    for receiver in RECEIVER_EMAILS:
        r = receiver.strip()
        if not r: continue
        try:
            msg = MIMEText(full_html, 'html', 'utf-8')
            msg['From'] = formataddr(("AI Insider", SENDER_EMAIL))
            msg['To'] = formataddr(("Reader", r))
            msg['Subject'] = Header(f"ğŸ”¥ ä»Šæ—¥AI: {datetime.now().strftime('%m/%d')} é‡ç‚¹æƒ…æŠ¥", 'utf-8')

            server = smtplib.SMTP_SSL("smtp.qq.com", 465)
            server.login(SENDER_EMAIL, EMAIL_PASSWORD)
            server.sendmail(SENDER_EMAIL, [r], msg.as_string())
            server.quit()
            print(f"âœ… å‘é€ç»™: {r}")
        except Exception as e:
            print(f"âŒ å‘é€å¤±è´¥ ({r}): {e}")


if __name__ == "__main__":
    items, new_history = fetch_data()
    if items:
        report = generate_newsletter(items)
        if report:
            send_email(report)
            save_history(new_history)
    else:
        print("ğŸ˜´ æ— æ–°å†…å®¹")
