import os
import json
import hashlib
import smtplib
import feedparser
import requests  # å¼•å…¥ requests ç”¨äºæ›´å¼ºçš„ä¼ªè£…
import time
from datetime import datetime
from openai import OpenAI
from email.mime.text import MIMEText
from email.header import Header
from email.utils import formataddr
from bs4 import BeautifulSoup

# ================= 1. å…¨å±€é…ç½® =================

API_KEY = os.environ.get("API_KEY")
API_BASE_URL = "https://api.deepseek.com"
SENDER_EMAIL = os.environ.get("SENDER_EMAIL")
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD")
RECEIVER_EMAIL = os.environ.get("RECEIVER_EMAIL")

HISTORY_FILE = "news_history.json"

# ğŸ”¥ ä¿®å¤åçš„ RSS æºåˆ—è¡¨ (ä½¿ç”¨ RSSHub é•œåƒæˆ–æ›´ç¨³å®šçš„æº)
RSS_SOURCES = {
    "Tech & AI": [
        "https://news.ycombinator.com/rss",  # Hacker News (æå°‘å°é”)
        # æ›¿æ¢ HF ä¸º ArXiv (CS.AI)ï¼Œè¿™æ˜¯è®ºæ–‡çš„æºå¤´ï¼Œä¸ä¼š 401
        "http://export.arxiv.org/rss/cs.AI",
        # OpenAI é€šå¸¸æ²¡æœ‰å®˜æ–¹ RSSï¼Œè¿™é‡Œä½¿ç”¨ç¬¬ä¸‰æ–¹èšåˆæˆ–å®˜æ–¹ Blog çš„ XML
        "https://openai.com/news/rss.xml",
        # æ›¿æ¢ Anthropic ä¸º TechCrunch AI æ¿å—ï¼Œæ›´ç¨³å®š
        "https://techcrunch.com/category/artificial-intelligence/feed/",
    ],
    "Global News": [
        # ä½¿ç”¨è·¯é€ç¤¾çš„ RSSHub é•œåƒ (å¦‚æœåŸç‰ˆè¢«å°) æˆ–è€…ç›´æ¥ä½¿ç”¨ Yahoo News (è·¯é€ç¤¾æº)
        "https://www.yahoo.com/news/rss/world",
        "http://feeds.bbci.co.uk/news/world/rss.xml",  # BBC ä¾ç„¶æ˜¯æœ€ç¨³å®šçš„
    ],
    "Science": [
        "https://www.sciencedaily.com/rss/top/science.xml",
        "https://www.nature.com/nature.rss"
    ]
}


# ================= 2. å·¥å…·å‡½æ•° =================

def get_hash(text):
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}


def save_history(history):
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def clean_html(html_content):
    if not html_content: return ""
    soup = BeautifulSoup(html_content, "html.parser")
    text = soup.get_text()
    # å»é™¤å¤šä½™ç©ºè¡Œ
    return " ".join(text.split())[:300].strip() + "..."


def fetch_rss_data():
    """ä½¿ç”¨ Requests + User-Agent ä¼ªè£…æŠ“å–"""
    print("ğŸŒ å¼€å§‹æŠ“å– (å·²å¯ç”¨åçˆ¬ä¼ªè£…)...")
    history = load_history()
    today_str = datetime.now().strftime("%Y-%m-%d")

    valid_history = {k: v for k, v in history.items()
                     if (datetime.now() - datetime.strptime(v, "%Y-%m-%d")).days < 5}

    collected_items = []

    # ğŸ•µï¸â€â™‚ï¸ å…³é”®ä¿®æ”¹ï¼šä¼ªè£…æˆ Chrome æµè§ˆå™¨
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
    }

    for category, urls in RSS_SOURCES.items():
        print(f"  ğŸ‘‰ æ­£åœ¨æ‰«æ: {category}...")
        for url in urls:
            try:
                # 1. å…ˆç”¨ requests ä¸‹è½½å†…å®¹ (ç»•è¿‡ç®€å•çš„ User-Agent å±è”½)
                response = requests.get(url, headers=headers, timeout=10)

                if response.status_code != 200:
                    print(f"    âš ï¸ è·³è¿‡ {url} (Status: {response.status_code})")
                    continue

                # 2. å†æŠŠä¸‹è½½åˆ°çš„å†…å®¹å–‚ç»™ feedparser
                feed = feedparser.parse(response.content)

                for entry in feed.entries[:3]:
                    link = entry.link
                    uid = get_hash(link)

                    if uid in valid_history:
                        continue

                    valid_history[uid] = today_str

                    # å°è¯•å¤šç§å­—æ®µè·å–æ‘˜è¦
                    raw_summary = getattr(entry, 'summary',
                                          getattr(entry, 'description', ''))

                    collected_items.append({
                        "category": category,
                        "title": entry.title,
                        "url": link,
                        "summary": clean_html(raw_summary),
                        "source_name": feed.feed.title if 'title' in feed.feed else "News"
                    })
            except Exception as e:
                print(f"    âŒ æŠ“å–é”™è¯¯ {url}: {e}")

    return collected_items, valid_history


# ================= 3. AI åˆ†ææ ¸å¿ƒ =================

def ai_analyze_report(items):
    print(f"ğŸ§  AI æ­£åœ¨åˆ†æ {len(items)} æ¡æƒ…æŠ¥...")
    if not items: return None

    input_text = ""
    for i, item in enumerate(items, 1):
        input_text += f"""
        ã€{i}ã€‘[{item['category']}] {item['title']}
        Link: {item['url']}
        Summary: {item['summary']}
        -----------------------------------
        """

    client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)

    # æç¤ºè¯
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆã€‚è¯·ä»ä»¥ä¸‹æ•°æ®ä¸­ç­›é€‰ 7-8 æ¡æœ€æœ‰ä»·å€¼çš„å…¨çƒæ–°é—»ï¼ˆä¾§é‡ç§‘æŠ€çªç ´å’Œå›½é™…å¤§äº‹ï¼‰ã€‚

    ã€è¾“å‡ºè¦æ±‚ã€‘
    1. ç›´æ¥è¾“å‡º HTML ä»£ç ã€‚
    2. æ¯ä¸€æ¡æ–°é—»ä½¿ç”¨ä¸‹é¢çš„ HTML æ¨¡æ¿ï¼Œä¸è¦æ”¹å˜ class åç§°ï¼š

    <div class="news-card">
        <div class="card-header">
            <span class="category-tag">ç±»åˆ«</span>
            <span class="source-tag">æ¥æº</span>
        </div>
        <h3 class="news-title"><a href="åŸæ–‡é“¾æ¥" target="_blank">ä¸­æ–‡æ ‡é¢˜</a></h3>
        <div class="news-content">
            <p><strong>ğŸ’¡ æ ¸å¿ƒäº‹å®ï¼š</strong> ç®€è¿°å‘ç”Ÿäº†ä»€ä¹ˆã€‚</p>
            <p><strong>ğŸ“¢ å½±å“åˆ†æï¼š</strong> è¿™ä»¶äº‹ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ</p>
        </div>
    </div>

    ã€åŸå§‹æ•°æ®ã€‘
    {input_text}
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=3000
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"âŒ AI æ¥å£é”™è¯¯: {e}")
        return None


# ================= 4. é‚®ä»¶å‘é€ =================

def send_email(html_content):
    print("ğŸ“§ æ­£åœ¨å‘é€é‚®ä»¶...")
    # CSS æ ·å¼ä¿æŒä¸å˜ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…è¿™é‡Œçœç•¥ï¼Œå¯ä»¥ç›´æ¥ç”¨ä¹‹å‰ä»£ç é‡Œçš„ CSS
    css = """
    <style>
        body { font-family: Helvetica, Arial, sans-serif; background: #f4f4f4; padding: 20px; }
        .container { max-width: 700px; margin: 0 auto; background: #fff; padding: 20px; border-radius: 8px; }
        .news-card { border-bottom: 1px solid #eee; margin-bottom: 20px; padding-bottom: 20px; }
        .news-title { font-size: 18px; margin: 10px 0; }
        .news-title a { color: #333; text-decoration: none; }
        .category-tag { background: #007bff; color: white; padding: 2px 5px; font-size: 12px; border-radius: 3px; }
        .footer { text-align: center; color: #888; font-size: 12px; margin-top: 20px; }
    </style>
    """

    html_body = f"""
    <html><head>{css}</head><body>
    <div class="container">
        <h2>ğŸŒ Global Daily Briefing ({datetime.now().strftime('%Y-%m-%d')})</h2>
        {html_content}
        <div class="footer">Powered by DeepSeek AI</div>
    </div>
    </body></html>
    """

    msg = MIMEText(html_body, 'html', 'utf-8')
    msg['From'] = formataddr(("DailyBot", SENDER_EMAIL))
    msg['To'] = formataddr(("Reader", RECEIVER_EMAIL))
    msg['Subject'] = Header(f"æ¯æ—¥ç®€æŠ¥ - {datetime.now().strftime('%m/%d')}", 'utf-8')

    try:
        server = smtplib.SMTP_SSL("smtp.qq.com", 465)
        server.login(SENDER_EMAIL, EMAIL_PASSWORD)
        server.sendmail(SENDER_EMAIL, [RECEIVER_EMAIL], msg.as_string())
        server.quit()
        print("âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")


if __name__ == "__main__":
    items, new_history = fetch_rss_data()
    if not items:
        print("ğŸ˜´ æ— æ–°å†…å®¹")
        exit(0)

    report = ai_analyze_report(items)
    if report:
        send_email(report)
        save_history(new_history)
